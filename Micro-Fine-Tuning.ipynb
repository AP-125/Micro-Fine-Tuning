{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2259b-5ef1-4c28-b0ac-d43f7b05ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "#The nightly which allows torch to use RTX 5090s.\n",
    "%pip install transformers\n",
    "\n",
    "from huggingface_hub import snapshot_download, login\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "token=\"...\"\n",
    "#Your Hugging Face token.\n",
    "login(token=token)\n",
    "\n",
    "local_dir = snapshot_download(\"google/gemma-2-9b-it\")\n",
    "print(\"Files are in:\", local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26715d2-6f5b-43eb-9ec8-a98648e8ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/root/.cache/huggingface/hub/models--google--gemma-2-9b-it/snapshots/11c9b309abf73637e4b6f9a3fa1e92e615547819\"\n",
    "#Or whatever path got printed in the previous cell.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16)\n",
    "\n",
    "rmsnorm_location = \"model.model.layers[21].post_feedforward_layernorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfeb482f-e687-4cc1-99b9-8716390dc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latents(texts: list[str], model, rmsnorm_location: str, tokenizer, device: torch.device | str = \"cpu\") -> list[torch.Tensor]:\n",
    "    model.to(device).eval()\n",
    "    captured_latents: list[torch.Tensor] = []\n",
    "\n",
    "    hook_location = eval(rmsnorm_location)\n",
    "    #This location should, of course, depend on the model architecture, but in general we want it right after a layernorm so that the latents all lie on a well-defined hyperellipsoid.\n",
    "    def _hook(module, inp, output):\n",
    "        captured_latents.append(output.clone().detach().cpu())\n",
    "    handle = hook_location.register_forward_hook(_hook)\n",
    "\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(**enc)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    hidden_states = captured_latents[0]\n",
    "    #Since we run all given texts together as a single batch, captured_latents will only have one element.\n",
    "    mask = enc.attention_mask.cpu().bool()\n",
    "    flat = hidden_states[mask]\n",
    "    #Getting rid of the padding tokens.\n",
    "\n",
    "    return [flat[i] for i in range(flat.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67a3103d-59b4-41f1-a843-c4f12b6f1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_layernorm_params(model, rmsnorm_location: str): \n",
    "    rms = eval(rmsnorm_location)\n",
    "\n",
    "    weight = rms.weight.detach().cpu()\n",
    "    \n",
    "    d = weight.numel()\n",
    "    radii = weight * (d ** 0.5)\n",
    "    #In layernorm, the normalized vector prior to multiplication by gamma weights has norm sqrt(d), and so the final latent ellipsoid has axis length 2*gamma*sqrt(d).\n",
    "\n",
    "    return radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43c1b9b9-edd4-41fb-96c6-e1999e6efcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_latent(x, normal, b, radii, alpha):\n",
    "    #We take in a latent x lying on an ellipsoid immediately post-layernorm, rescale it to the unit sphere, add some factor of the normal direction, then scale it back to the ellipsoid.\n",
    "    #If we know normal is in the semantic direction of some concept we're trying to steer the model towards, this should \"insert\" that concept into the latent.\n",
    "    x_sph = x / radii\n",
    "    #x_sph should now lie on the unit sphere.\n",
    "    s = torch.relu((x_sph * normal).sum(dim=1) + b)\n",
    "    delta = (alpha - s).unsqueeze(1)\n",
    "    x_mod = x_sph + delta * normal.unsqueeze(0)\n",
    "\n",
    "    return x_mod * radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f2e772e-2c62-4ffd-a556-6a9ff313205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PlaneLearner(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.n_raw = nn.Parameter(torch.randn(dim))\n",
    "        #The vector normal to the plane.\n",
    "        self.theta = nn.Parameter(torch.tensor(0.0))\n",
    "        #If the plane is n.x+b=0, then b=tanh(theta).\n",
    "\n",
    "    def forward(self):\n",
    "        n = F.normalize(self.n_raw.float(), dim=0)\n",
    "        b = torch.tanh(self.theta.float())\n",
    "        return n, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b25d0b84-08ec-4458-846c-5978f23a9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_loss(unit_vecs: torch.Tensor, n: torch.Tensor, b: torch.Tensor):\n",
    "    #This loss is more-or-less the (negative) proportion of unit vectors which lie on the desired side of the plane n.x+b=0; that is, in the desired spherical cap.\n",
    "    n = n.float()\n",
    "    b = b.float()\n",
    "    unit_vecs = unit_vecs.float()\n",
    "    logits  = unit_vecs @ n + b\n",
    "    scores  = torch.sigmoid(logits)\n",
    "    #The sigmoid is a smooth approximation of an indicator function.\n",
    "    return -scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ace78011-5da4-4c21-a4d4-26bedd8b94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "def mle_loss_on_example(model,\n",
    "    tokenizer, rmsnorm_location: str,\n",
    "    example_texts: List[str],\n",
    "    n: torch.Tensor, b: torch.Tensor,\n",
    "    radii: torch.Tensor,\n",
    "    alpha: float\n",
    "):\n",
    "    device, dtype = n.device, n.dtype\n",
    "\n",
    "    text = random.choice(example_texts)\n",
    "    #We choose a random string among the given texts to perform autoregression on.\n",
    "    ids = tokenizer(text, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "    cut = random.randint(1, ids.numel() - 2)\n",
    "    #Then we choose a random token in that string to try to predict.\n",
    "    prefix_ids = ids[:cut].unsqueeze(0).to(device)\n",
    "    target_id  = ids[cut].to(device)\n",
    "\n",
    "    rms = eval(rmsnorm_location)\n",
    "    radii_ = radii.to(device, dtype=dtype)\n",
    "    n_ = n.to(device, dtype=dtype)\n",
    "    b_ = b.to(device, dtype=dtype)\n",
    "\n",
    "    def hook(_m, _inp, out):\n",
    "        last  = out[:, -1, :]\n",
    "        out[:, -1, :] = modify_latent(last, n_, b_, radii_, alpha)\n",
    "        return out\n",
    "\n",
    "    h = rms.register_forward_hook(hook)\n",
    "    logits = model(prefix_ids).logits\n",
    "    h.remove()\n",
    "\n",
    "    log_probs = F.log_softmax(logits[0, -1], dim=-1)\n",
    "    loss = -log_probs[target_id]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ad61036-3300-41ee-a5c6-2574560b2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def train_plane(\n",
    "    model, tokenizer, rmsnorm_location: str,              \n",
    "    example_texts,\n",
    "    radii,\n",
    "    alpha,\n",
    "    use_averaging: bool,\n",
    "    num_steps=500, batch_size=8, lr=5e-4,\n",
    "    lambda_cap=0.1, lambda_theta=0.1,\n",
    "    lambda_mle=1e4,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    model = model.to(device).eval()\n",
    "    dtype = torch.float32\n",
    "    d = model.get_input_embeddings().embedding_dim\n",
    "\n",
    "    if use_averaging:\n",
    "        latents = extract_latents(example_texts, model, rmsnorm_location, tokenizer, device)\n",
    "        unit_vecs_list = [latent.to(device) / radii.to(device) for latent in latents]\n",
    "        unit_vecs = torch.stack(unit_vecs_list)\n",
    "    \n",
    "    plane = PlaneLearner(d).to(device)\n",
    "    optim = torch.optim.Adam(plane.parameters(), lr=lr)\n",
    "\n",
    "    for step in range(1, num_steps + 1):\n",
    "        n, b = plane()\n",
    "        theta = torch.acos((-b).clamp(-0.999, 0.999))\n",
    "\n",
    "        loss_theta = lambda_theta * theta if use_averaging else 0\n",
    "        #Encourages a smaller spherical cap.\n",
    "        loss_cap = cap_loss(unit_vecs, n, b) if use_averaging else 0\n",
    "        #Encourages the spherical cap to contain more of the example latents.\n",
    "        loss_mle = mle_loss_on_example(model, tokenizer, rmsnorm_location, example_texts, n, b, radii, alpha)\n",
    "        #Encourages the spherical cap to move in a semantically meaningful direction.\n",
    "\n",
    "        loss = lambda_cap * loss_cap + loss_theta + lambda_mle * loss_mle\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            plane.n_raw[:] = F.normalize(plane.n_raw, dim=0)\n",
    "\n",
    "        if step % 10 == 0 or step == 1:\n",
    "            if use_averaging:\n",
    "                print(\n",
    "                    f\"{step}  L_cap={loss_cap.item():.4f}  \"\n",
    "                    f\"b={b.item():.6f}  MLE={loss_mle.item():.4f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"{step}  MLE={loss_mle.item():.4f}\")\n",
    "\n",
    "    n_final, b_final = plane()\n",
    "    return n_final.detach(), b_final.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ca005a7-24b1-4603-8fb0-b480ae03de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_modified_model(\n",
    "    model, tokenizer,\n",
    "    rmsnorm_location: str,\n",
    "    input_text: str,\n",
    "    alpha: float,\n",
    "    radii,\n",
    "    normal, b,\n",
    "    max_new_tokens: int = 200,\n",
    "    device: str = \"cpu\"\n",
    "):\n",
    "    model = model.to(device).eval()\n",
    "    norm_mod = eval(rmsnorm_location)\n",
    "    radii = radii.to(device)\n",
    "    normal = normal.to(device)\n",
    "    b = b.to(device)\n",
    "\n",
    "    def hook_fn(module, inp, output):\n",
    "        out  = output.clone()\n",
    "        last = out[:, -1, :]\n",
    "        with torch.no_grad():\n",
    "            out[:, -1, :] = modify_latent(last, normal, b, radii, alpha)\n",
    "        return out\n",
    "\n",
    "    handle = norm_mod.register_forward_hook(hook_fn)\n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    gen_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    handle.remove()\n",
    "    return tokenizer.decode(gen_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4b2b9-dff1-4728-baa2-09848a0d75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "example_texts = [\n",
    "    \"Thou art kind.\",\n",
    "    \"Wherefore cam’st thou hither?\",\n",
    "    \"He doth protest too much.\",\n",
    "    \"Get thee to a nunnery, and quickly, for the day grows short.\",\n",
    "    \"Fain would I go, yet duty binds me here.\",\n",
    "    \"’Tis but thy name that is my enemy; thou art thyself, though not a Montague.\",\n",
    "    \"Methinks it is the east, and Juliet is the sun.\",\n",
    "    \"Come hither, good sir, and lend thine ear unto my counsel.\",\n",
    "    \"Thy will be done.\",\n",
    "    \"Wouldst thou leave me so unsatisfied, when night itself doth call thee home?\",\n",
    "    \"Hast thou no pity left in thy breast, no drop of mercy?\",\n",
    "    \"Speak’st thou in jest, or is thy meaning earnest?\",\n",
    "    \"I know not where he lies, nor whence he came, yet something in his countenance speaks truth.\",\n",
    "    \"Let us hence; this place grows cold with treachery.\",\n",
    "    \"So foul and fair a day I have not seen.\",\n",
    "    \"Had I but followed mine own counsel, this misfortune had ne’er befallen us.\",\n",
    "    \"I prithee, stay a while, for the moon is yet high and I have more to tell.\",\n",
    "    \"This night methinks is wondrous strange, full of portents and whisperings.\",\n",
    "    \"Be it known unto all that here I stand, resolute in word and deed.\"\n",
    "]\n",
    "\"\"\"\n",
    "#example_texts=[\"Oh, yes, there's no place I'd rather be than here with you, answering your brilliant questions that never seem to end.\"]\n",
    "example_texts = [\"I love weddings!\", \"Let's get married...\", \"Who will be your best man?\", \"You may kiss the bride.\"]\n",
    "radii         = extract_layernorm_params(model, rmsnorm_location)\n",
    "latent_vecs   = extract_latents(example_texts, model, rmsnorm_location, tokenizer, \"cuda\")\n",
    "\n",
    "n, b = train_plane(\n",
    "    model, tokenizer,\n",
    "    rmsnorm_location = rmsnorm_location,\n",
    "    example_texts    = example_texts,\n",
    "    radii            = radii,\n",
    "    alpha            = 1.0,\n",
    "    use_averaging    = False,\n",
    "    num_steps        = 500,\n",
    "    batch_size       = 8,\n",
    "    lr               = 5e-4,\n",
    "    lambda_cap       = 1e-1,\n",
    "    lambda_theta     = 1e-1,\n",
    "    lambda_mle       = 1e4,\n",
    "    device           = \"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a41fd-2927-4655-9db8-0c033f04793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_modified_model(\n",
    "        model, tokenizer,\n",
    "        rmsnorm_location = rmsnorm_location,\n",
    "        input_text       = \"Let me tell you a story:\",\n",
    "        alpha            = 3.0,\n",
    "        radii            = radii,\n",
    "        normal           = n,\n",
    "        b                = b,\n",
    "        max_new_tokens   = 200,\n",
    "        device           = \"cuda\"\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ee1a8-2aa8-4c23-b714-235389ec8858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
